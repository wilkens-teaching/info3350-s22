{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "400fe12e-0c3f-4ae1-9bab-7051b5903739",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 26: Working with Social Media Data\n"
    "\n"
    "Delivered by [Federica Bologna](https://federicabologna.github.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e8b47d-b40a-4243-8bcc-252de1f54fd0",
   "metadata": {},
   "source": [
    "## What is social media data?\n",
    "\n",
    " * Collected from social networks\n",
    " * Different kinds:\n",
    "     * Example: posts, comments, likes, followers, clicks, shares (reposts and retweets), comments.\n",
    "     * Numerical or textual format\n",
    "     * We'll focus on textual data\n",
    "\n",
    "![](images/socialmediadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f74934b-2f64-4fbf-9113-15102abac712",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Why is working with social media data important?\n",
    " \n",
    " * Literature can tell us about the past\n",
    "     * Look backward in time\n",
    "     * We don't have a lot of digitized textual data from the past (letters? birth certificates?)\n",
    " * Social media data can tell us about the present moment\n",
    "     * Look at the present or forward in time\n",
    "     * We have lots of data from social networks!\n",
    " * It can tell us about\n",
    "     * human behaviour\n",
    "     * language\n",
    "     * current events\n",
    "\n",
    "![](images/interestingpapers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea00c80-be11-4f61-a8e4-3298e0ae3c7c",
   "metadata": {},
   "source": [
    "## Differences between literary texts and texts from social media\n",
    " \n",
    " * Literary texts\n",
    "     * Historical\n",
    "     * Long format\n",
    "     * Formally edited and published\n",
    " * Text from social media data\n",
    "     * Contemporary (more or less)\n",
    "     * Shorter format\n",
    "     * Not formally edited and published"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66056b9-9bd3-4491-af84-309c45c2c529",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Do students at Cornell talk about student life differently in 2020 vs 2022?\n",
    "\n",
    "For the scope of this exercise, we will only focus on Reddit posts and comments published in March and April of 2020, and in March and April of 2022.\n",
    "To investigate this question we will:\n",
    "- Scrape posts and comments;\n",
    "- Gather information about the corpus of post and comments;\n",
    "- Deduplicate and clean the corpus;\n",
    "- Perform topic modeling;\n",
    "- Evaluate topic modeling;\n",
    "- Perform classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bf82a7-139c-428c-a2c9-17c2a265913e",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561c8eef-d90b-4822-a2a1-209426476825",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set up scraping\n",
    "\n",
    "Install some new packages for this lecture. We have to use `pip`, since none of these are available via `conda`.\n",
    "\n",
    "**Note that `tomotopy` does not work natively on Apple Silicon Macs.** If you're running python via Rosetta, you'll be fine. If you're running M1-native python, you're out of luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "036349d1-2c97-4f16-978c-0ab195700a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psaw\n",
      "  Downloading psaw-0.1.0-py3-none-any.whl (15 kB)\n",
      "Collecting little_mallet_wrapper\n",
      "  Downloading little_mallet_wrapper-0.5.0-py3-none-any.whl (19 kB)\n",
      "Collecting Levenshtein\n",
      "  Downloading Levenshtein-0.18.1-cp310-cp310-macosx_10_9_x86_64.whl (242 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.9/242.9 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/mwilkens/opt/miniconda3/envs/3350/lib/python3.10/site-packages (from psaw) (2.27.1)\n",
      "Requirement already satisfied: Click in /Users/mwilkens/opt/miniconda3/envs/3350/lib/python3.10/site-packages (from psaw) (8.1.3)\n",
      "Collecting rapidfuzz<3.0.0,>=2.0.1\n",
      "  Downloading rapidfuzz-2.0.11-cp310-cp310-macosx_10_9_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting jarowinkler<1.1.0,>=1.0.2\n",
      "  Downloading jarowinkler-1.0.2-cp310-cp310-macosx_10_9_x86_64.whl (71 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/mwilkens/opt/miniconda3/envs/3350/lib/python3.10/site-packages (from requests->psaw) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/mwilkens/opt/miniconda3/envs/3350/lib/python3.10/site-packages (from requests->psaw) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/mwilkens/opt/miniconda3/envs/3350/lib/python3.10/site-packages (from requests->psaw) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mwilkens/opt/miniconda3/envs/3350/lib/python3.10/site-packages (from requests->psaw) (3.3)\n",
      "Installing collected packages: little_mallet_wrapper, jarowinkler, rapidfuzz, psaw, Levenshtein\n",
      "Successfully installed Levenshtein-0.18.1 jarowinkler-1.0.2 little_mallet_wrapper-0.5.0 psaw-0.1.0 rapidfuzz-2.0.11\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install psaw little_mallet_wrapper Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62cb97e5-baae-4291-aeb4-0e1235475063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tomotopy\n",
      "  Downloading tomotopy-0.12.2.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /Users/mwilkens/opt/miniconda3/envs/3350/lib/python3.10/site-packages (from tomotopy) (1.21.6)\n",
      "Building wheels for collected packages: tomotopy\n",
      "  Building wheel for tomotopy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tomotopy: filename=tomotopy-0.12.2-cp310-cp310-macosx_10_9_x86_64.whl size=4515277 sha256=9a88c661e2aa95a09cd8a4f047c3651174ca4b891d58d709baaf83c46b964bef\n",
      "  Stored in directory: /Users/mwilkens/Library/Caches/pip/wheels/5b/d0/39/bbf899953bc8cca3118d45475ca70f690bea75621962de26d5\n",
      "Successfully built tomotopy\n",
      "Installing collected packages: tomotopy\n",
      "Successfully installed tomotopy-0.12.2\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install tomotopy # does not work on M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53963f8a-260e-4636-b558-37959a6ffbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "base_path = os.path.join('reddit_data')  # creating a directory for the data\n",
    "if not os.path.exists(base_path):  # if it does not exist\n",
    "    os.makedirs(base_path)         # create it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9df937-4b29-4eff-a1de-75f7377518ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scraping functions\n",
    "\n",
    "Here are the two functions for scraping posts and comments respectively from the subreddit of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68bc9710-d62e-4c12-b879-032c49139269",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Maria Antoniak's code with minor modifications \"\"\"\n",
    "def scrape_posts_from_subreddit(subreddit, api, year, month, end_date):\n",
    "    '''\n",
    "    Takes the name of a subreddit, the PushshiftApi, a year and month to scrape from\n",
    "    '''\n",
    "    start_epoch = int(datetime(year, month, 1).timestamp())  # convert date into unicode timestamp\n",
    "    end_epoch = int(datetime(year, month, end_date).timestamp())\n",
    "\n",
    "    gen = api.search_submissions(after=start_epoch,\n",
    "                                 before=end_epoch,\n",
    "                                 subreddit=subreddit,\n",
    "                                 filter=['url', 'author', 'created_utc',  # info we want about the post\n",
    "                                         'title', 'subreddit', 'selftext',\n",
    "                                         'num_comments', 'score', 'link_flair_text', 'id'])\n",
    "\n",
    "    max_response_cache = 100000\n",
    "    scraped_posts = []\n",
    "    for _post in gen:\n",
    "        scraped_posts.append(_post)\n",
    "        if len(scraped_posts) >= max_response_cache:  # avoid requesting more posts than allowed\n",
    "            break\n",
    "\n",
    "    scraped_posts_df = pd.DataFrame([p.d_ for p in scraped_posts])\n",
    "\n",
    "    return scraped_posts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63161418-5d26-405b-be31-2a7b122c622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Maria Antoniak's code with minor modifications \"\"\"\n",
    "def scrape_comments_from_subreddit(subreddit, api, year, month, end_date):\n",
    "    '''\n",
    "    Takes the name of a subreddit, the PushshiftApi, a year and month to scrape from\n",
    "    '''\n",
    "    start_epoch = int(datetime(year, month, 1).timestamp())  # convert date into unicode timestamp\n",
    "    end_epoch = int(datetime(year, month, end_date).timestamp())\n",
    "\n",
    "    gen = api.search_comments(after=start_epoch,\n",
    "                              before=end_epoch,\n",
    "                              subreddit=subreddit,\n",
    "                              filter=['author', 'body', 'created_utc', # info we want about the comment\n",
    "                                      'id', 'link_id', 'parent_id',\n",
    "                                      'reply_delay', 'score', 'subreddit'])\n",
    "\n",
    "    max_response_cache = 100000\n",
    "    scraped_comments = []\n",
    "    for _comment in gen:\n",
    "        scraped_comments.append(_comment)\n",
    "        if len(scraped_comments) >= max_response_cache:  # avoid requesting more posts than allowed\n",
    "            break\n",
    "    scraped_comments_df = pd.DataFrame([p.d_ for p in scraped_comments])\n",
    "\n",
    "    return scraped_comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af1fd6-f78a-48c3-bb61-20ce6ddc35b2",
   "metadata": {},
   "source": [
    "### Scrape!\n",
    "\n",
    "Here we will decide:\n",
    "- which subreddit to scrape,\n",
    "- which content type to scrape from that subreddit,\n",
    "- and which dates we want to scrape.\n",
    "And we will set off the previous scraping functions accordingly.\n",
    "\n",
    "We will save files to **pickle format**, why?\n",
    "- To avoid confusion when reading and writing them! Texts contain commas, and it is possible that pandas might read them as separators when reading CSV files.\n",
    "\n",
    "NOTE ON DIRECTORIES:\n",
    "- Our jupyter notebook is in a folder on our machine\n",
    "  - inside that folder we previously we created a folder `reddit_data`\n",
    "    - inside `reddit_data` we will create a folder named after the subreddit we will scrape `Cornell`\n",
    "      - inside `Cornell` we will create one folder for each of the two content types `posts` and `comments`\n",
    "        - inside `posts` we will store all the data about the posts of the Cornell subreddit\n",
    "        - inside `comments` we will store all the data about the comments of the Cornell subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c084d28c-39c4-47f1-bffc-555436826232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Maria Antoniak's code with minor modifications \"\"\"\n",
    "def scrape_subreddit(_target_subreddits, _target_types, _years):\n",
    "    '''\n",
    "    Takes a list of subreddits, a list of types of content to scrape, and a list of years to scrape from\n",
    "    '''\n",
    "    \n",
    "    api = PushshiftAPI()\n",
    "\n",
    "    print('Number of PushshiftApi shards that are not working:', api.metadata_.get('shards'))  # check if any Pushshift shards are down!\n",
    "    \n",
    "    for _subreddit in _target_subreddits:\n",
    "        for _target_type in _target_types:\n",
    "            for _year in _years:\n",
    "                if _year < 2022:\n",
    "                    months = [3, 4]\n",
    "                    end_dates = [31, 30]\n",
    "                elif _year == 2022:\n",
    "                    months = [3, 4]  # months to scrape\n",
    "                    end_dates = [31, 30]  # last day of the month\n",
    "\n",
    "                for _month, _end_date in zip(months, end_dates):\n",
    "                    _output_directory_path = os.path.join(base_path, _subreddit, _target_type)  # directory to store scraped data\n",
    "                                                                                                # by subreddit and type of content\n",
    "                    if not os.path.exists(_output_directory_path):  # if it does not exist\n",
    "                        os.makedirs(_output_directory_path)         # create it!\n",
    "\n",
    "                    _file_name = _subreddit + '-' + str(_year) + '-' + str(_month) + '.pkl'  # filename of the csv with scraped data\n",
    "\n",
    "                    # scrape only if output file does not already exist\n",
    "                    if _file_name not in os.listdir(_output_directory_path):\n",
    "\n",
    "                        print(str(datetime.now()) + ' ' + ': Scraping r/' + _subreddit + ' ' + str(_year) + '-' + str(_month) + '...')\n",
    "\n",
    "                        if _target_type == 'posts':\n",
    "                            _posts_df = scrape_posts_from_subreddit(_subreddit, api, _year, _month, _end_date)\n",
    "                            if not _posts_df.empty:\n",
    "                                _posts_df.to_pickle(os.path.join(_output_directory_path, _file_name), protocol=4)\n",
    "\n",
    "                        if _target_type == 'comments':\n",
    "                            _comments_df = scrape_comments_from_subreddit(_subreddit, api, _year, _month, _end_date)\n",
    "                            if not _comments_df.empty:\n",
    "                                _comments_df.to_pickle(os.path.join(_output_directory_path, _file_name), protocol=4)\n",
    "\n",
    "    print(str(datetime.now()) + ' ' + ': Done scraping!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11f70c83-c566-4783-9c16-a2a84addd1de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PushshiftApi shards that are not working: None\n",
      "2022-05-03 17:30:49.391005 : Done scraping!\n"
     ]
    }
   ],
   "source": [
    "target_subreddits = ['cornell']  # subreddits to scrape\n",
    "target_types = ['posts', 'comments']  # type of content to scrape\n",
    "years = [2020, 2022]  # years to scrape\n",
    "scrape_subreddit(target_subreddits, target_types, years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60915b6e-7c31-4a22-af62-a477de7f96ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Combine posts and comments for one subreddit\n",
    "\n",
    "Here we will combine the pickle files with all the posts from the subreddit and the pickle files with all the comments from the same subreddit into one file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c4b9bd5-7f2c-4859-a4af-071667218989",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_one_subreddit(_subreddit):  # creating csv with all of a subreddit's posts and comments\n",
    "\n",
    "    df_d = {'author': [], 'id': [], 'type': [], 'text': [],   # create a dictionary\n",
    "            'url': [], 'link_id': [], 'parent_id': [],\n",
    "            'subreddit': [], 'created_utc': []}\n",
    "    \n",
    "    subreddit_pkl_path = os.path.join('reddit_data', _subreddit, f'{_subreddit}.pkl') # file with all the data\n",
    "    if not os.path.exists(subreddit_pkl_path):  # if file does not exist\n",
    "        \n",
    "        for target_type in ['posts', 'comments']:\n",
    "            files_directory_path = os.path.join('reddit_data', _subreddit, target_type)  # directory where scraped data is depending on subreddit and type of content\n",
    "            all_target_type_files = glob.glob(os.path.join(files_directory_path, \"*.pkl\"))  # select all appropriate pickle files\n",
    "            for f in all_target_type_files:  # we read each pickle file and include the info we want in the dictionary\n",
    "                df = pd.read_pickle(f)\n",
    "\n",
    "\n",
    "                if target_type == 'posts':\n",
    "                    for index, row in df.iterrows():\n",
    "                        df_d['author'].append(row['author'])\n",
    "                        df_d['id'].append(f\"{row['subreddit']}_{row['id']}_post\")  # id of the post, 'Endo_xyz123_post'\n",
    "                        df_d['type'].append('post')\n",
    "                        df_d['text'].append(row['selftext'])  # textual content of the post\n",
    "                        df_d['url'].append(row['url'])  # url of the post\n",
    "                        df_d['link_id'].append('N/A')\n",
    "                        df_d['parent_id'].append('N/A')\n",
    "                        df_d['subreddit'].append(row['subreddit'])\n",
    "                        df_d['created_utc'].append(row['created_utc'])  # utc time stamp of the post\n",
    "\n",
    "\n",
    "                elif target_type == 'comments':\n",
    "                    for index, row in df.iterrows():\n",
    "                        df_d['author'].append(row['author'])\n",
    "                        df_d['id'].append(f\"{row['subreddit']}_{row['id']}_comment\")\n",
    "                        df_d['type'].append('comment')\n",
    "                        df_d['text'].append(row['body'])  # textual content of the comment\n",
    "                        df_d['url'].append(f\"http://www.reddit.com/r/Endo/comments/{row['link_id'].split('_')[1]}/\")  # url of the post\n",
    "                        df_d['link_id'].append(row['link_id'])\n",
    "                        df_d['parent_id'].append(row['parent_id'])\n",
    "                        df_d['subreddit'].append(row['subreddit'])\n",
    "                        df_d['created_utc'].append(row['created_utc'])  # utc time stamp of the post\n",
    "\n",
    "\n",
    "        subreddit_df = pd.DataFrame.from_dict(df_d)  # create pandas dataframe from dictionary\n",
    "        subreddit_df.sort_values('created_utc', inplace=True, ignore_index=True)  # order dataframe by date of post\n",
    "        subreddit_df['time'] = pd.to_datetime(subreddit_df['created_utc'], unit='s').apply(lambda x: x.to_datetime64())  # convert timestamp to date\n",
    "        subreddit_df['date'] = subreddit_df['time'].apply(lambda x: str(x).split(' ')[0])\n",
    "        subreddit_df['year'] = subreddit_df['time'].apply(lambda x: str(x).split('-')[0])\n",
    "        subreddit_df.drop(columns=['time'])\n",
    "        \n",
    "        subreddit_df.to_pickle(subreddit_pkl_path, protocol=4)  # saving it to pickle format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3311db3f-bb0c-4f9d-9cf3-0f93f30caba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for subreddit in target_subreddits:\n",
    "    combine_one_subreddit(subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c5f81-6ee9-4576-b71d-6c937843a81c",
   "metadata": {},
   "source": [
    "## Some info on the corpus\n",
    "\n",
    "Before performing any analysis it's important to get to know our texts. Characteristics about our social media texts affect how we will carry out our analysis. Let's check:\n",
    "- how long the texts are,\n",
    "- how many words are in the vocabulary of the corpus,\n",
    "- what the most commons words are in the corpus etc.\n",
    "\n",
    "This information will inform how we will clean the texts and perform topic modeling on them in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73b627e5-9622-46c8-8639-30e2ee054da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0baac51b-74f2-4fd8-b2ca-677f8889c4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60023\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(os.path.join('reddit_data', 'cornell', 'cornell.pkl'))\n",
    "df = df.dropna()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c6e7fe9-3870-4df0-8fe0-6061ee757f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(df, _type):\n",
    "    if _type != 'corpus':\n",
    "        vectorizer = CountVectorizer(        # Token counts with stopwords\n",
    "            input = 'content',               # input is a string of texts\n",
    "            encoding = 'utf-8',\n",
    "            strip_accents = 'unicode',\n",
    "            lowercase = True\n",
    "        )\n",
    "\n",
    "        texts = df['text'].astype('string').tolist()\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        print(f\"Total vectorized words in the corpus of {_type}:\", X.sum())\n",
    "        print(f\"Average vectorized {_type} length:\", int(X.sum()/X.shape[0]), \"tokens\")\n",
    "    \n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            input = 'content',\n",
    "            encoding = 'utf-8',\n",
    "            strip_accents = 'unicode',\n",
    "            lowercase = True,\n",
    "            stop_words = 'english'          # remove stopwords\n",
    "        )\n",
    "        \n",
    "        texts = df['text'].astype('string').tolist()\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        sum_words = X.sum(axis=0)\n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "        words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        print('Top words in the combined corpus of posts and comments after removing stopwords:')\n",
    "        for word, freq in words_freq[:30]:\n",
    "            print(word, '\\t', freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29ee9650-0e5a-4552-983f-82f113cfcd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts in r/Cornell: 7310\n",
      "Number of comments in r/Cornell: 52713\n",
      "Number of posts and comments from 2020 in r/Cornell: 22160\n",
      "Number of posts and comments from 2022 in r/Cornell: 37863\n",
      "Total vectorized words in the corpus of posts: 283923\n",
      "Average vectorized posts length: 38 tokens\n",
      "Total vectorized words in the corpus of comments: 1362887\n",
      "Average vectorized comments length: 25 tokens\n"
     ]
    }
   ],
   "source": [
    "df_posts = df.loc[df['type'] == 'post'].copy()\n",
    "df_comments = df.loc[df['type'] == 'comment'].copy()\n",
    "df_2020 = df.loc[df['year'] == '2020'].copy()\n",
    "df_2022 = df.loc[df['year'] == '2022'].copy()\n",
    "print(f'Number of posts in r/Cornell:', len(df_posts))\n",
    "print(f'Number of comments in r/Cornell:', len(df_comments))\n",
    "print(f'Number of posts and comments from 2020 in r/Cornell:', len(df_2020))\n",
    "print(f'Number of posts and comments from 2022 in r/Cornell:', len(df_2022))\n",
    "print_info(df_posts, 'posts')\n",
    "print_info(df_comments, 'comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66fe64db-c4d7-470d-a2a0-163be9b22b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in the combined corpus of posts and comments after removing stopwords:\n",
      "just \t 8224\n",
      "cornell \t 8148\n",
      "like \t 7191\n",
      "people \t 6822\n",
      "don \t 6465\n",
      "think \t 4714\n",
      "know \t 4409\n",
      "time \t 4061\n",
      "really \t 3925\n",
      "class \t 3718\n",
      "students \t 3680\n",
      "good \t 3184\n",
      "classes \t 3178\n",
      "want \t 3172\n",
      "ve \t 3113\n",
      "https \t 2970\n",
      "school \t 2892\n",
      "make \t 2570\n",
      "year \t 2502\n",
      "going \t 2469\n",
      "semester \t 2463\n",
      "ll \t 2433\n",
      "work \t 2428\n",
      "need \t 2356\n",
      "housing \t 2335\n",
      "campus \t 2318\n",
      "got \t 2310\n",
      "lot \t 2291\n",
      "way \t 2121\n",
      "sure \t 2047\n"
     ]
    }
   ],
   "source": [
    "print_info(df, 'corpus')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfe1dda-0584-43c2-91b9-03f2677b3eb6",
   "metadata": {},
   "source": [
    "## Pre-process the corpus\n",
    "\n",
    "\n",
    "When scraping Reddit or other platforms, it is important to consider how the platform is used by users, to have an idea of the kind of texts we might find.\n",
    "\n",
    "A few things to keep in mind:\n",
    "- the content on these platforms is **barely curated**. Moderators and bots designed for content moderation often just remove the most offensive and inflammatory content.\n",
    "  - Unless you are dealing with a special subreddit/community that enforces very strict norms, you will find funky looking, uninformative, and bot-generated texts.\n",
    "- In most social platforms, social interaction can revolve around **images**. Unless alt-text is provided (sadly, basically never), we cannot access that information using our NLP tools.\n",
    "  - Therefore some texts will look funky for that reason. Such documents are generally short.\n",
    "- On Reddit, content shows up depending on the up- and down-votes it receives. If a user's post gets ignored by their community, they sometime repost it to receive an answer.\n",
    "  - Thus, in your corpus, you might find 5, 10, 20 **duplicates** of an individual post.\n",
    "\n",
    "HOWEVER, how much and whether you need to clean your corpus highly depends on **a few factors**:\n",
    "- The goal of your analysis, your question\n",
    "- The community you are analyzing\n",
    "  - **Be respectful!** This content might look weird to you, but can mean a lot to the members of the community\n",
    "  - Keep in mind that you are analyzing someone's behavior and interaction online. Put yourself in their shoes :)\n",
    "- The techniques you are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c45cac83-ff70-4407-ba15-fbd964417050",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import little_mallet_wrapper as lmw\n",
    "import Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5aced7-f176-4ab8-8b72-103c3af1f370",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Deduplicating function\n",
    "\n",
    "This is far from an optimal function for getting rid of duplicates. For sake of time, we will make sure that content posted by the same user is not duplicated, and that the previous post - chrnologically - is not identical.\n",
    "\n",
    "We will use the Levenshtein distance. It measures how different two strings are. It is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one string into the other. It is useful because it does not require tokenization. So we can get rid of most of the duplicates before cleaning the data, saving us some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "814e69aa-eb57-4d7e-a064-8afae63042bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(_df):  # function to find duplicated posts in the data\n",
    "\n",
    "    prev_doc = ''\n",
    "    map_dict = {}  # dict of authors' posts\n",
    "    duplicate_indexes = []  # list of duplicates' indexes for removal from dataframe\n",
    "    for index, row in _df.iterrows():  # iterate over posts\n",
    "        author = row['author']\n",
    "        doc = row['text']\n",
    "\n",
    "        # if author info is available we compare each post with previous ones by the same author\n",
    "        # we compare/calculate the similarity between the posts using the Levenshtein distance\n",
    "        if author != '[deleted]':\n",
    "            if author in map_dict.keys():\n",
    "                flag = 0\n",
    "                idx = 0\n",
    "                while idx < len(map_dict[author]) and flag == 0:\n",
    "                    lev = Levenshtein.ratio(doc, map_dict[author][idx])\n",
    "                    if lev > 0.99:\n",
    "                        duplicate_indexes.append(index)\n",
    "                        flag = 1\n",
    "                    idx += 1\n",
    "                if flag == 0:\n",
    "                    map_dict[author].append(doc)\n",
    "            else:\n",
    "                map_dict[author] = [doc]\n",
    "\n",
    "        # if author info is not available we compare each post with the preceding one chronologically\n",
    "        else:\n",
    "            lev = Levenshtein.ratio(row['text'], prev_doc)\n",
    "            if lev > 0.90:\n",
    "                duplicate_indexes.append(index)\n",
    "\n",
    "        prev_doc = doc\n",
    "\n",
    "    return duplicate_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42b574a4-68b6-4362-bcf8-46af69cdf14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates: 1453\n"
     ]
    }
   ],
   "source": [
    "dupes = find_duplicates(df)  # find duplicates\n",
    "df.drop(dupes, inplace=True)  # removing duplicates\n",
    "print(f'Number of duplicates: {len(dupes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d1d11-a710-448e-bb8c-49d1c240f883",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Cleaning function\n",
    "\n",
    "Before we perform topic modeling it's important we remove messages generated by bots or that are not diverse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "604002d5-7fe8-4655-8e56-18bd02360538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_docs(raw_df, _subreddit):\n",
    "    '''\n",
    "    Takes the full corpus, a file path. It cleans all the documents (removes punctuation and stopwords). It saves the clean corpus in a json file\n",
    "    '''\n",
    "    clean_docs_file = os.path.join('reddit_data', _subreddit, f'clean_{_subreddit}.pkl')\n",
    "    if not os.path.exists(clean_docs_file): \n",
    "        \n",
    "        clean_d = {'id':[], 'clean':[], 'og':[], 'year':[], 'date':[]}\n",
    "\n",
    "        for index, row in raw_df.iterrows():                               # iterating over posts and comments\n",
    "            if 'bot' not in row['author'] and 'Bot' not in row['author']:  # if author is not a bot\n",
    "                clean_doc_st = lmw.process_string(row['text'])             # cleaning documents\n",
    "                clean_doc_l = [t for t in clean_doc_st.split(' ')]\n",
    "                if len(set(clean_doc_l))>5 and 'bot' not in clean_doc_l:  # exclude posts that have less than 5 different words\n",
    "                                                                          # or that contain word 'bot'\n",
    "                    clean_d['clean'].append(clean_doc_l)\n",
    "                    clean_d['id'].append(row['id'])\n",
    "                    clean_d['og'].append(row['text'])\n",
    "                    clean_d['year'].append(row['year'])\n",
    "                    clean_d['date'].append(row['date'])\n",
    "\n",
    "        with open(clean_docs_file, 'w') as jsonfile:  # creating a file with the dict of documents to topic model\n",
    "            json.dump(clean_d, jsonfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fca7154-40d4-4bd6-b20c-f5c1297c5232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 235 µs, sys: 124 µs, total: 359 µs\n",
      "Wall time: 333 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for subreddit in target_subreddits:\n",
    "        cleaning_docs(df, subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3567cd5-5f10-4d09-9ed0-42b178f9a498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Topic modeling\n",
    "\n",
    "What topics appear in Cornell's subreddit?\n",
    "\n",
    "To perform LDA, we will be using `tomotopy` a new, fast and easy-to-use package for topic modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4681ba0-2bf4-4304-a574-434e18b681ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tomotopy as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5fd834-6d78-400f-944c-50f30106c933",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Topic modeling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2bb5f97-7e9e-439c-bc4f-726cb906fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Mixture of Matthew Wilkens' and Melanie Walsh's code\"\"\"\n",
    "def perform_topic_modeling(_doc_ids, _clean_docs, _num_topics, _rm_top, _topwords_file):\n",
    "    '''\n",
    "    Takes a list of document ids, a list of clean docs to perform LDA on, a number of topics, a number of top words to remove,\n",
    "    a file path for the top words file. It performs topic modeling on the documents, then creates the top words file and a doc-term matrix.\n",
    "    '''\n",
    "                                          # setting and loading the LDA model\n",
    "    lda_model = tp.LDAModel(k=_num_topics,      # number of topics in the model\n",
    "                            min_df=3,           # remove words that occur in less than n documents\n",
    "                            rm_top=_rm_top)     # remove n most frequent words\n",
    "    for doc in _clean_docs:\n",
    "        lda_model.add_doc(doc)  # adding document to the model\n",
    "\n",
    "    iterations = 10\n",
    "    for i in range(0, 100, iterations):  # train model 10 times with 10 iterations at each training = 100 iterations\n",
    "        lda_model.train(iterations)\n",
    "        print(f'Iteration: {i}\\tLog-likelihood: {lda_model.ll_per_word}')\n",
    "\n",
    "    # Writing the document with the TOP WORDS per TOPIC\n",
    "    num_top_words = 25                                      # number of top words to print for each topic\n",
    "    with open(_topwords_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(f\"\\nTopics in LDA model: {_num_topics} topics {_rm_top} removed top words\\n\\n\")\n",
    "                                                            # write settings of the model in file\n",
    "        topic_individual_words = []\n",
    "        \n",
    "        for topic_number in range(0, _num_topics):                  # for each topic number in the total number of topics\n",
    "            topic_words = ' '.join(                                 # string of top words in the topic\n",
    "                word for word, prob in lda_model.get_topic_words(topic_id=topic_number, top_n=num_top_words))\n",
    "                                                # get_topic_words is a tomotopy function that returns a dict of words and their probabilities\n",
    "            \n",
    "            topic_individual_words.append(topic_words.split(' '))   # append list of the topic's top words for later\n",
    "            file.write(f\"Topic {topic_number}\\n{topic_words}\\n\\n\")  # write topic number and top words in file\n",
    "\n",
    "            \n",
    "    # TOPIC DISTRIBUTIONS\n",
    "    topic_distributions = [list(doc.get_topic_dist()) for doc in lda_model.docs]  # list of lists of topic distributions for each document\n",
    "    topic_results = []\n",
    "    for topic_distribution in topic_distributions:\n",
    "        topic_results.append({'topic_distribution': topic_distribution}) # adding dicts of topic distributions to list\n",
    "    \n",
    "    df = pd.DataFrame(topic_results, index=_doc_ids) \n",
    "                                                    # df where each row is the list of topic distributions of a document, s_ids are the ids of the sentences\n",
    "    column_names = [f\"Topic {number} {topic[0]}\" for number, topic in enumerate(topic_individual_words)]  # create list of column names from topic numbers and top words\n",
    "    \n",
    "    df[column_names] = pd.DataFrame(df['topic_distribution'].tolist(), index=df.index)\n",
    "                                    # df where topic distributions are not in a list and match the list of column names\n",
    "    df = df.drop('topic_distribution', axis='columns')  # drop old topic distributions' column\n",
    "    \n",
    "    dominant_topic = np.argmax(df.values, axis=1)       # get dominant topic for each document\n",
    "    df['dominant_topic'] = dominant_topic\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38fbfa4f-b0ed-4672-acb3-d17b19a0b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_topic_modeling(_subreddit):\n",
    "    tomo_folder = os.path.join('output', 'topic_modeling')  # results' folder\n",
    "    if not os.path.exists(tomo_folder):  # create folder if it doesn't exist\n",
    "        os.makedirs(tomo_folder)\n",
    "    \n",
    "    clean_docs_file = os.path.join('reddit_data', _subreddit, f'clean_{_subreddit}.pkl')\n",
    "    with open(clean_docs_file) as json_file:\n",
    "        clean_docs_dict = json.load(json_file)\n",
    "    doc_ids = clean_docs_dict['id']       # list of ids of clean documents\n",
    "    clean_docs = clean_docs_dict['clean']  # list of clean documents to perform topic modeling on                       \n",
    "\n",
    "    \n",
    "    print(\"Performing Topic Modeling...\")     # for loop to run multiple models with different settings with one execution\n",
    "    for num_topics in [10, 20]:            # for number of topics\n",
    "        for rm_top in [5]:                 # for number of most frequent words to remove\n",
    "\n",
    "            topwords_file = os.path.join(tomo_folder, f'{subreddit}-{num_topics}_{rm_top}.txt')  # path for top words file\n",
    "            docterm_file = os.path.join(tomo_folder,f'{subreddit}-{num_topics}_{rm_top}.pkl')  # path for doc-topic matrix file\n",
    "            if not os.path.exists(topwords_file) or not os.path.exists(docterm_file):         # if result files don't exist, performs topic model\n",
    "                \n",
    "                start = datetime.now()\n",
    "                lda_dtm = perform_topic_modeling(doc_ids, clean_docs, num_topics, rm_top, topwords_file)\n",
    "                lda_dtm['og_doc'] = clean_docs_dict['og']    # list of original documents for evaluation\n",
    "                lda_dtm['year'] = clean_docs_dict['year']\n",
    "                lda_dtm['date'] = clean_docs_dict['date']\n",
    "                lda_dtm.to_pickle(docterm_file, protocol=4)  # convert doc-topic df in csv file\n",
    "                print(f'{str(datetime.now())}____Topic modeling {num_topics}, {rm_top} time:____{str(datetime.now() - start)}\\n')  # print timing of topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422f604d-bce4-4780-ac58-6a85a4ec3985",
   "metadata": {},
   "source": [
    "### Run Topic Modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accd7365-eca3-45d6-82a4-bab31ca692eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Topic Modeling...\n",
      "CPU times: user 206 ms, sys: 65.6 ms, total: 272 ms\n",
      "Wall time: 325 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for subreddit in target_subreddits:\n",
    "    run_topic_modeling('cornell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568c7fd-5f5d-4164-9832-23dbd297de93",
   "metadata": {},
   "source": [
    "### Evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bb54479-820a-4693-94e2-52b4b3745690",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "\n",
    "def print_top_docs_per_topic(_df, _txtfile):\n",
    "    \n",
    "    with open(_txtfile, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        idx = 3\n",
    "        while idx < len(lines):\n",
    "            topic_line = lines[idx]\n",
    "            words_line = lines[idx+1]\n",
    "            n = topic_line.split()[1]\n",
    "            word_1 = words_line.split()[0]\n",
    "            print(f'{topic_line}{words_line}')\n",
    "            for doc in _df.sort_values(f'Topic {n} {word_1}', ascending=False).og_doc.tolist()[5:10]:\n",
    "                print(doc)\n",
    "                print(\"_________\")\n",
    "            print('\\n\\n')\n",
    "            idx += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6318aeb-85d0-44e1-8ac2-6edacd657033",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in topic model: 34217\n",
      "Topic 0\n",
      "year semester time last classes years summer next fall first take one since online two think got much would didn still ago start freshman really\n",
      "\n",
      "If he had done well in first semester, he would have been in a different college by the start of second semester anyway\n",
      "_________\n",
      "I don't know how it works for all labs but at least for bio labs the first semester is usually the slowest and most chill since you can't do much. So in general it's better to start research in the summer because you can get a lot more training done and be useful in the lab faster. But if it's between next semester or summer next year then you might as well start next semester and get even more done summer next year.\n",
      "_________\n",
      "you could do classes? Honestly the summer after freshman year is the last opportunity you have to really chill during the summer (until graduation tbh). It's up to you how you want to spend the time with regards to your education timeline. Nothing is super certain about the summer right now.\n",
      "_________\n",
      "Is this official? And will PE classes still be held for the next two weeks?\n",
      "_________\n",
      "Yo relax and have some fun, I had 2 C’s and the rest of my grades were B’s in my second semester of senior year. Never was contacted by Cornell… you should be A-OK!!\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 1\n",
      "campus ithaca live single housing west room living house looking dorm apartment one year would dorms nyc rent north roommate double city collegetown interested new\n",
      "\n",
      "My friend and I are looking for a third roommate next year. Apartment has 3 bedrooms, kitchen, and bathroom and is ~800 a month in Collegetown. Looking for a clean quiet female lol\n",
      "_________\n",
      "Hey, I have a female double in ganedago, looking for a single \\[that isn't South or Dickson\\]. Willing to pay for the swap\n",
      "_________\n",
      "Hi, I have a single in Ganedago (female floor) on the fourth floor and I am looking to swap with a double preferably in Ganedago or Morrison.\n",
      "_________\n",
      "My friends and I are looking for one more roommate for our 4 person collegetown apartment next year, rent if $995/month and the lease is for a year starting in june, dm me if interested:)\n",
      "_________\n",
      "Dickson triple, looking to swap for a single in ganedago, morrison or West campus main building /s\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 2\n",
      "know want really going feel need would time life make one right anyone trying way even let someone anything better much else stay take could\n",
      "\n",
      "Agreed. Find someone who will support you and try to stay around people. You can always call the hotlines. If you feel like you're in danger, take yourself to the ER. I don't know you but your life has worth and value. Keep pushing. Life really doesn't feel that way forever ❤\n",
      "_________\n",
      "I WANT YOU TO BE ALIVE\n",
      "\n",
      "YOU DONT GOTTA DIE\n",
      "\n",
      "AND LEMME TELL YOU WHY\n",
      "_________\n",
      "take care of yourself and stay safe!! things are hard rn but it'll get better :))\n",
      "_________\n",
      "This helped me a lot. And I don’t think anyone here is a failure, I’m just very critical of myself and never feel like I’m enough.\n",
      "\n",
      "I’m sorry if I gave off a bad impression, I’d never want to make anyone feel bad or judge others.\n",
      "_________\n",
      "UGH, I am furious for you as the sister of someone with T1D. I'm so sorry this happened to you. It's hard enough without stuff like this happening. Hope you can get things sorted out quickly and easily.\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 3\n",
      "food dining money pay use eat plan need cost meal tuition also buy halls used free paid much price yes brbs NUMk refund using laptop\n",
      "\n",
      "But I want meat. And fries. But even grilled chicken breasts, burgers, beef without a lot of sauces, chicken and beef. Just plain food. Pizza is spicy to me. I eat it here and there because there is very little I like most nights. So pasta and pizza. . I think grill foods nightly in every dining hall is a necessity.\n",
      "_________\n",
      "describe lasagna without using the words “pasta, sauce, marinara, ricotta, cheese, layers, mozzarella, Italy, Italian”\n",
      "_________\n",
      "I’m going to get an iPad Air, Apple Pencil, and Smart Keyboard for notes and the like, but what laptop should I get? Would a MacBook Air M1 be enough? Will I need a monitor?\n",
      "_________\n",
      "There’s around $3000 in the total cost of attendance that isn’t directly billed by the bursar (for textbooks and misc expenses). So assuming you live on-campus, the amount you pay directly to bursar will be pretty much actually the parent contribution. That’s probably where OP is coming from.\n",
      "_________\n",
      "iPad is unnecessary. You can substitute it with a slew of other tablet options that may be better depending on use case (Surface Pro, Surface Laptop Studio, even those tablets that try to mimic paper, like the Remarkable.)\\*\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 4\n",
      "email students please post student university event group law information said speech free support send protest sent new department freedom community posts would contact emails\n",
      "\n",
      "here’s info 1260! \n",
      "\n",
      "You're invited to my new group 'INFO 1260 / CS 1340 Spring '22' on GroupMe. Click here to join: https://groupme.com/join_group/84761582/YIhSXAyH\n",
      "_________\n",
      "The speaker Jordan Lawrence is part of the Alliance Defending Freedom which is classified as an anti-LGBT hate group. Read the basics here, but do ur own research ofc [https://en.wikipedia.org/wiki/Alliance\\_Defending\\_Freedom](https://en.wikipedia.org/wiki/Alliance_Defending_Freedom)\n",
      "_________\n",
      "So saying “trans rights are human rights” is unconstitutional? Because that is the bullying they are referring to.\n",
      "_________\n",
      "Locked. Please direct all anti-mod posts to the mod megathread\n",
      "_________\n",
      "You're invited to my new group 'Cornell University Class of 2024' on GroupMe. Click here to join: https://groupme.com/join_group/56511863/CFaPKEmx\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 5\n",
      "covid mask masks virus cases wear risk still sick testing health test spread flu wearing tested person case medical even coronavirus disease one positive pandemic\n",
      "\n",
      "It's the norm in countless countries outside the US to wear a mask if you're sick in public spaces as to not spread it to other people. It's been like that even before COVID. Why not do this?\n",
      "_________\n",
      "1. people who die from flu generally contract it in hospitals, not at the grocery store. \n",
      "\n",
      "2. they are not less important. many dying from COVID-19 are people who are out and about with a relatively minor or well-managed health issue (like hypertension, pacemaker, diabetes). The flu wouldn't have anywhere near as high of a chance to kill them.\n",
      "_________\n",
      "DING DONG DING DONG DONG DING DING DONG\n",
      "\n",
      "\n",
      "DONG DONG DONG DONG DONG DONG DONG DONG DONG DONG\n",
      "\n",
      "\n",
      "A prize for whoever translates correctly\n",
      "_________\n",
      "Ithacans need to get vaccinated and boosted. Just like students did. Then they wouldn’t overwhelm the healthcare system or die in high numbers. Responsibility is on the unvaccinated at this point. At least until a variant comes along that isn’t ameliorated by the vaccine .\n",
      "_________\n",
      "I will wear mask and get 3 vaccines \n",
      "\n",
      "You need to wear mask and get 3 vaccines ...\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 6\n",
      "room also hall bus building library night campus please around one free open water use lost anyone store outside used found right minutes fire walking\n",
      "\n",
      "Anyone know where to find free outdoor tennis courts near or on campus?\n",
      "_________\n",
      "Will you pick up and put my boxes in the storage squad locker I rented? I’m not doing the free option.\n",
      "_________\n",
      "If you lost keys attached to a red Cornell lanyard with a Jamaica keychain on them on North campus, they’re in the RPCC mail center’s lost and found now.\n",
      "_________\n",
      "There are a few parking spots by forest home drive and you can easily walk in! Open to foot traffic but closed to cars. Pro tip: there are some nice trails over the creek bridge and also along the forest home drive side towards varna!\n",
      "_________\n",
      "Please please please get a water bottle and just refill at the many water fountains around campus. Your wallet and the environment will thank you. 48 plastic bottles a week = 2500 plastic bottles a year!!!! Most plastic bottles don't get recycled, please don't do that. On top of that bottled water is not as clean as you think it is. Everyone I know and see at Cornell uses a reusable water bottle.\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 7\n",
      "think know would even really say someone actually make one sure didn lol doesn probably said post mean much maybe something person yeah though question\n",
      "\n",
      "Yeah. I thought that was clearly implied. Don’t know why I’m getting downvoted.\n",
      "_________\n",
      "I didn’t know for sure so I asked a question. Is there something wrong with that?\n",
      "_________\n",
      "(If there is, I’m not sure if there are) They would legally have to make exceptions, so not something you have to worry about.\n",
      "_________\n",
      "I didn’t find either easy lol.....or even moderate\n",
      "_________\n",
      "lmao thanks for pointing out! I realized the mistake but was too lazy to edit it🤪\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 8\n",
      "day got one lol shit good never see guy man love every literally back thought ever went today slope nothing would look yeah time hear\n",
      "\n",
      "I straight drove here from KS and stopped in Ohio at like midnight and got Cane's.... Such a surreal memory\n",
      "_________\n",
      "I literally didn't even realize I had bulimia until I saw this post. I go through like \\~3-4 days of extreme binge eating followed by depression/panic attacks and then don't eat for about 5 days. Its had the same affects that you described but I didn't realize it had a name and thought my profs would just laugh at me if I reached out about it.\n",
      "_________\n",
      "A man walked into a bar and shat his pants. When the bartender asked for a drink, the man said, “I hardly know her!”\n",
      "_________\n",
      "My NY ass came to the PNW and used an umbrella... That lasted for one day. Now I walk in the rain like a psycho all winter.\n",
      "_________\n",
      "a good setting powder was a literal game changer for me as someone who’s face gets super oily after a night out! in my pictures at oweek parties i look like a shiny goblin but my friend bought me some mid fall semester and now i can be super sweaty and gross but i wack that shit on and still look fine!\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 9\n",
      "really school friends lot also good think college much life great things ithaca feel find experience pretty make social many best better big high would\n",
      "\n",
      "CS at Cornell is competitive but it's also the largest major...\n",
      "\n",
      "There are tons of poor, average, and excellent people at Cornell in terms of academic performance and that comes from the fact that Cornell tries to draw from different walks of life, and some people don't necessarily come to college just to be the best in academics and that's cool too.\n",
      "_________\n",
      "Most of the things I do are outdoor related. If you're into outdoorsy things, this is the place to be. Hiking, running, cycling, backpacking, skiing, and so so much more is really easy to do around here and there's a large portion of people and organizations surrounding those interests\n",
      "_________\n",
      "I love the ECE department here, it's really good. I remember also debating between GT and Cornell, and I definitely do not regret coming here.\n",
      "\n",
      "Social life is what you make of it. Joining clubs, greek life, talking to ppl in your dorm/classes are all good ways to find your friend group(s). If you're talking social in terms of parties, then some clubs will have internal parties and greek life will have some but there's a crackdown on greek life.\n",
      "\n",
      "I think it's p diverse.\n",
      "_________\n",
      "Don't agree.\n",
      "Can't say that I dont really know much about it if I've been living here for a good minute. I drive everywhere and explore what I can because I'm a huge outdoor person, and I've come to the conclusion that it offers nothing. Unless you consider no sun, snow, potholes, a choice between 3 bars, 1 \"night\"club, no escape rooms, etc. the most amazing thing ever, then so be it.\n",
      "_________\n",
      "With Martha’s new regulations “open” parties are pretty much non-existing now. 90% of Greek events are mixers now, so if you are a guy you pretty much have to rush (if you’re a girl you can go to you’re friends mixers) to go to frat parties. Other organizations do have parties as well but they’re less common and not as big as Greek parties. So if you are a guy and into the party scene you probably should rush.\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 10\n",
      "campus back break day home anyone ithaca spring would come know time april leave stay NUMth may students going open week last days NUMpm need\n",
      "\n",
      "There were a ton of flight issues where mine got delayed and I missed the flight into Ithaca so now I'm flying into Syracuse. Would anyone be willing to drive me from syracuse into campus tonight? Will be willing to pay $100 :)\n",
      "_________\n",
      "looking for options to come back to campus, departing from Syracuse airport 8PM April 6th! Let me know if there's anyone having the same schedule who wants to share a ride. Or any suggestions would be wonderful because there's no bus option late at night. Willing to pay if someone is driving at this time too!\n",
      "_________\n",
      "Anyone driving to Maryland/DC/Virginia either Thursday night or Friday and have a spot in their car? My ride home fell through :( I'll pay for gas and bring snacks!\n",
      "_________\n",
      "need to go 4/3 anytime and come back 4/11 after 10pm but ourbus doesn’t allow pets /:\n",
      "\n",
      "ik uber/lyft but does anyone know of any other options?\n",
      "_________\n",
      "Hi I’m looking for a ride to Syracuse tomorrow (3/16) departing from Ithaca before 12PM. I am open to splitting an Uber or taxi, or whatever tbh.\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 11\n",
      "take classes class credits major credit pre requirements would one med courses course requirement need want count also info semester taking enroll drop advisor fws\n",
      "\n",
      "i am in 14 academic credits, with 1 class s/u so there is technically only 10 graded credits.  does this fulfill the 12 academic credit minimum?\n",
      "_________\n",
      "Could I knock out an Info Sci minor with this pathway:\n",
      "\n",
      "**Information science major with Data science concentration: One core class based on concentration (INFO 2950 in this case), three courses from the same concentration, elective from any concentration**\n",
      "\n",
      "CS 4780 double counting as a 4000+ CS elective and Data Science IS elective\n",
      "\n",
      "CS 3300 double counting as a 3000+ CS technical elective and Data Science IS elective\n",
      "\n",
      "INFO 3/4000something double counting as a 3000+ external specialization and Data Science IS elective\n",
      "\n",
      "INFO 2450 double counting as a liberal arts and elective from any concentration\n",
      "\n",
      "INFO 2950 double counting as an advisor approved elective and core concentration class\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "If I could do this this would be the snipe of the century lmao\n",
      "_________\n",
      "I need to fulfill the CALS distribution requirement and I suck at science. I don't know which course to take.\n",
      "_________\n",
      "can any of the \"three 4000+ CS Electives each at three credits\" contribute to the technical elective?? \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "also, can BTRY 3080 fulfill probability requirement and specialization?\n",
      "_________\n",
      "Yeah I know. I’m doing chem e and most classes are shared between premed requirements and major requirements based on what I’ve researched like we need to do some bio electives and organic chem, etc for premed.\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 12\n",
      "https //www reddit view poll utm google source app link share png youtube html xNUMb medium com/watch format pdf name com/ auto website width redd\n",
      "\n",
      "[https://www.youtube.com/watch?v=aBABQEhtjOs&amp;feature=youtu.be&amp;fbclid=IwAR1M0nb77doxuuLk9hsHCF5IH3CQ-3DHRPt8-NUQeU7NvxVq2GDB3XbDeJQ](https://www.youtube.com/watch?v=aBABQEhtjOs&amp;feature=youtu.be&amp;fbclid=IwAR1M0nb77doxuuLk9hsHCF5IH3CQ-3DHRPt8-NUQeU7NvxVq2GDB3XbDeJQ)\n",
      "_________\n",
      "Tell us again how Trump listens to experts...\n",
      "\n",
      "[https://www.politico.com/news/2020/03/25/trump-coronavirus-national-security-council-149285](https://www.politico.com/news/2020/03/25/trump-coronavirus-national-security-council-149285)\n",
      "_________\n",
      "https://www.reddit.com/r/Cornell/comments/t65hze/ibc_is_stinky/?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf\n",
      "_________\n",
      "[https://statements.cornell.edu/2020/20200310-coronavirus-update.cfm](https://statements.cornell.edu/2020/20200310-coronavirus-update.cfm)\n",
      "_________\n",
      "https://www.cornell.edu/coronavirus/statements-news/20200315-research.cfm?utm_source=share&amp;utm_medium=ios_app&amp;utm_name=iossmf\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 13\n",
      "students s/u grades would semester student gpa universal grade think school opt system point schools mandatory policy option grading everyone grad pass pandemic choice take\n",
      "\n",
      "For the first paragraph essentially my point is that there exists standardization, (which is what I'm advocating for). Standardization of resources in the previous suffering case, and standardization of grades in this current case we are in.\n",
      "\n",
      "As for 'fixing injustice with injustice', the way I see it, one is proposing that we let those who have suffered in the past fix their GPA by getting good grades due to the circumstances of those suffering now. It is undoubtedly easier to get an A, due to the leniency of grading and the fact that the curved system will favor to those who *aren't* affected.\n",
      "_________\n",
      "My opinion is that we can more easily handle internal matters at cornell such as major affiliation because the records of grades will be local and easily accessible. Whereas with graduate school admissions this would not be the case, and to compare an S/U student and an A student from the same university externally is going to be unfair.\n",
      "_________\n",
      "Exactly I think universal pass is moronic. I clearly have the advantage to get an A over many others who will just end up with S. What on earth would compel me to give that up?\n",
      "_________\n",
      "Not to be selfish but I'm not going to graduate school and probably will have a low GPA this semester so I would really like to have universal S/U. However, what is going on with it right now? When do we expect an official decision? Is there a decision we highly expect to be made? Once it is Universal, is there no way people will be able to discover their number grades?\n",
      "_________\n",
      "I disagree: rather, the arguments for universal pass/fail were incredibly weak imo. The bulk of the support in the first half was bc Harvard med school would only accept pass fail if it was universally mandated: however, as it was later pointed out, this situation is rapidly evolving and it seems schools keep changing their stance as well. I forgot who said this but I agree with this faculty member: I can’t see any student being penalized for taking something s/u. The letter that was posted to the transcript by professor Birman was especially powerful in my opinion. Big red pass kind of implements itself as the voice for FGLI students, while in reality, I’m sure many want this opportunity to raise GPAs. \n",
      "\n",
      "One more thing: the thing is GPAs are cumulative. One semester less to raise this will severely hurt many of us. An extended universal pass fail option is the only way to ensure flexibility for all students.\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 14\n",
      "school students student aid financial program college year got schools grad research also high would good undergrad apply top accepted gpa money NUMk transfer admissions\n",
      "\n",
      "What is the CoL-adjusted stipend rate for Cornell vs. other top schools for the upcoming year?\n",
      "_________\n",
      "ecornell - they have remote work. $16/hr ik a lot of people who just got hired and u can do it over the summer\n",
      "_________\n",
      "Also, Berkeley does not have a medical school. That statement refers to its graduate programs, for which gpa is much less important and given much less weight in comparison to MD/DO admissions.\n",
      "_________\n",
      "I’m a Financial economics major that has been accepted to Cornell, while I’m grateful to be admitted by the school I want to know from the student body if you all would think that going to a school with such prestige outweighs getting internships and such in my field. My main alternatives are currently Baruch College, Fordham University and pending for NYU, all schools that have good internship programs and and are in NYC, the epicenter of where I want to work and my home.\n",
      "_________\n",
      "I did med school with a 3.2 (but I got a masters degree first and a 99th percentile MCAT score first). \n",
      "\n",
      "PhD grad program with a 3.2\n",
      "\n",
      "Not in industry (high gpa shouldn’t be needed for industry).\n",
      "\n",
      "Your portfolio and how you think are perhaps more important.\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 15\n",
      "class classes professor time hours work prelim took one exam really course professors also lecture final grade exams week lectures semester last material much hard\n",
      "\n",
      "Hi, I took genetics lecture and lab together last semester. Like the person before me mentioned, practice problems are key in genetics lecture. Do as many of them as you can as possible. In lab, most of the assignments you can complete by going to office hours and asking questions. For exams in genetics lab, I went over all the assignments and PowerPoint presentation as some of the test questions can be some form of a question you will see on an assignment. I would like to add 1 more thing : office hours are key in both lecture and lab. Dr. Blake, one of the professors who does lecture and lab, is really good at explaining concepts and giving you tips on how to approach problems. If you go to office hours and do every single practice problem you can get your hands on, you will do great.\n",
      "_________\n",
      "I took it with orgo 1. It wasn’t impossible as long as you went to TA office hours for lab report and mechanism help. The final was known to be a bit difficult, so I memorized every mechanism and ended the class with a good grade. So if orgo lab works better with your fall schedule, I would just take it then.\n",
      "_________\n",
      "attendance is taken at lectures and labs and id recommend going to lab bc the TAs really help\n",
      "_________\n",
      "Take 1110 with Thomas. Her course is super organized so you can just do the homework’s/read the textbook and never show up to lecture (which like less than half the class attends) and get an A.\n",
      "_________\n",
      "Prelecture video+prelecture quiz+ coop+weekly in class quiz+weekly pset +lab+ lecture+all of this due and a quiz on the week of the prelim= 🤬⌛️😭🤮🌊🪦🚨\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 16\n",
      "time even work would many much think one money students still family every isn going job life make real part world working made less lot\n",
      "\n",
      "Wtf is wrong with you? Seriously it’s hard to come across this unlikable on the internet\n",
      "_________\n",
      "Makes no sense that dependents under 17 will give parents an extra $500 but none over 18...\n",
      "_________\n",
      "Ok? But they are in college rn. That is not enough time for family connections nor inheritance to make them money to the point you can call them rich themselves.\n",
      "_________\n",
      "Parents get $500 per child 17 years old or younger. College students do not qualify as children.\n",
      "_________\n",
      "At my job the software architect makes 300k for a start up with ~100 people. If Martha isn’t making at least half a million a year I’d be shocked\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 17\n",
      "chinese data think women asian different white also china use language many world social government race way action non learn based human would political say\n",
      "\n",
      "What I meant to say is that the criterion you use “actively fighting to reduce people’s rights” to condemn it that speaker is misguided. There is a multitude of political activists on all parts of the political spectrum who are actively trying to reduce all sorts of groups’ rights. Some of which you probably agree with. Word your reasoning better.\n",
      "_________\n",
      "What. I’m not saying every single little aspect of economics is ideological. Im saying that economics is fundamentally ideological. Which is true. The foundation and entire point of economics is the economy, which is an ideological and political matter.\n",
      "_________\n",
      "Where do you suggest self learning relativistic classical field theory from?\n",
      "_________\n",
      "No objection from me because it appears that white women benefit the most from affirmative action.\n",
      "_________\n",
      "I agree with beliefs 2 and 3, strongly disagree with 5, disagree with 4 and I am neutral on 1. My $.02\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 18\n",
      "think make way know help might feel need could sure time really good best see professor health well also try ask right something professors understand\n",
      "\n",
      "I know the situation may be difficult to emotionally unpack, so I suggest using Cornell Health’s mental health services or any other private entity that may be able to assist in your emotions.\n",
      "_________\n",
      "The most glaring issue I see with this entire situation is that you love fors\n",
      "_________\n",
      "Yeah. Not all professors are the same. Your statement is quite short and you're leaving others to conjecturing. You need to provide more substance for others to help you with advice instead of assuming we know all about you. Hint: we don't know you. We don't know what stress you are going through and we don't know what the assignment is whether easy or arduous.\n",
      "_________\n",
      "I think if the rec letters are coming from profs who know you well, they'll be fine with it. I guess just explain to them the situation, like you just did.\n",
      "_________\n",
      "It's one thing to disagree with someone but it's another thing to shame someone like this\n",
      "_________\n",
      "\n",
      "\n",
      "\n",
      "Topic 19\n",
      "major classes take math would engineering courses physics course class also want taking know science taken majors transfer thanks econ anyone level much really lot\n",
      "\n",
      "I'm trying to decide between ECE 4760 (Digital Systems Design Using Microcontrollers) and ECE 5725 (Design with Embedded Operating Systems) for my ECE minor. I want to take ECE 5725 since the stuff about Linux seems more interesting, but I haven't taken ECE 3400 and don't have that much hardware experience. ECE 4760 is also project-based and only requires ECE 3140/CS 3420, but I don't know how different it is compared to the other course. Would I be better off taking ECE 4760 if I just want hands-on experience, or should I try to go for ECE 5725?\n",
      "_________\n",
      "Hi! I’m an entomology major looking to pursue a career in science communication. I’m planning on taking the scicomm minor, but I was also wondering if adding an infosci minor would be helpful as well. If it’s worth it, are there any concentrations/courses that you’d recommend?\n",
      "_________\n",
      "Hey, I'm trying to decide what prob. class to take. My other classes are CS 3110, MATH 1920, INFO 2040, and ENGRI 1270. I am interested in ML and have taken AP stats and CS 2800. Is BTRY 3080 rigorous enough for ML? Will I die taking 4710 with my other classes? Thanks\n",
      "_________\n",
      "As a non-major interested in fashion, I'm considering taking either FSAD 1250Fashion, Art and Design Thinking or FSAD 1170 Fashion Graphics.\n",
      "\n",
      "Would appreciate it if anyone can share about the grading, workload, and content!!\n",
      "_________\n",
      "Hi im an incoming freshman thinking of majoring in statistical science and am just wondering how hard or time consuming the major is. (Because i plan to double major in econs too)\n",
      "\n",
      "Opinions from current stats majors (?)\n",
      "_________\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_removed = 5\n",
    "n_topics = 20\n",
    "tomo_folder = os.path.join('output', 'topic_modeling')\n",
    "tomo_pklfile = os.path.join(tomo_folder, f'cornell-{n_topics}_{n_removed}.pkl')\n",
    "tomo_txtfile = os.path.join(tomo_folder, f'cornell-{n_topics}_{n_removed}.txt')\n",
    "tomo_df = pd.read_pickle(tomo_pklfile)\n",
    "print(f'Number of documents in topic model: {len(tomo_df)}')\n",
    "print_top_docs_per_topic(tomo_df, tomo_txtfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a03b5-1125-40bc-86bb-d772512c41cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification using topic distributions\n",
    "\n",
    "Let's check if we can predict whether a post/comment is from 2020 or 2022 using its topic distributions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ac73799-cb41-4d80-b5c9-4ca0406fb9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c955f878-bc4d-44b0-994d-801deed74a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the performance of our simple classifiers\n",
    "# Freebie function to summarize and display classifier scores\n",
    "def compare_scores(scores_dict):\n",
    "    '''\n",
    "    Takes a dictionary of cross_validate scores.\n",
    "    Returns a color-coded Pandas dataframe that summarizes those scores.\n",
    "    '''\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(scores_dict).T.applymap(np.mean).style.background_gradient(cmap='RdYlGn')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29505b4d-332f-4999-90e5-f6b3c6279aa5",
   "metadata": {},
   "source": [
    "### 20 Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63af5853-0de4-488f-b44b-a74e8a1738a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tomo_shuffled = tomo_df.sample(frac=1)\n",
    "\n",
    "tomo_shuffled['y_year'] = tomo_shuffled['year'].apply(lambda x: 0 if x == '2020' else 1)\n",
    "y_year = tomo_shuffled['y_year'].tolist()\n",
    "x_docterm = tomo_shuffled[tomo_df.columns[:n_topics].tolist()]\n",
    "X_docterm = StandardScaler().fit_transform(x_docterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f64449cf-b036-48ed-94a6-da642e745b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6006078849694596"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline accuracy (guess 2022 every time)\n",
    "np.sum(y_year)/len(y_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c08cb94-2db2-4fa7-a27e-a67d4d6e665b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3b31a_row0_col0, #T_3b31a_row0_col1, #T_3b31a_row0_col3, #T_3b31a_row1_col2, #T_3b31a_row1_col4, #T_3b31a_row1_col5 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3b31a_row0_col2, #T_3b31a_row0_col4, #T_3b31a_row0_col5, #T_3b31a_row1_col0, #T_3b31a_row1_col1, #T_3b31a_row1_col3 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3b31a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3b31a_level0_col0\" class=\"col_heading level0 col0\" >fit_time</th>\n",
       "      <th id=\"T_3b31a_level0_col1\" class=\"col_heading level0 col1\" >score_time</th>\n",
       "      <th id=\"T_3b31a_level0_col2\" class=\"col_heading level0 col2\" >test_accuracy</th>\n",
       "      <th id=\"T_3b31a_level0_col3\" class=\"col_heading level0 col3\" >test_precision</th>\n",
       "      <th id=\"T_3b31a_level0_col4\" class=\"col_heading level0 col4\" >test_recall</th>\n",
       "      <th id=\"T_3b31a_level0_col5\" class=\"col_heading level0 col5\" >test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3b31a_level0_row0\" class=\"row_heading level0 row0\" >Logit</th>\n",
       "      <td id=\"T_3b31a_row0_col0\" class=\"data row0 col0\" >0.046522</td>\n",
       "      <td id=\"T_3b31a_row0_col1\" class=\"data row0 col1\" >0.016599</td>\n",
       "      <td id=\"T_3b31a_row0_col2\" class=\"data row0 col2\" >0.626473</td>\n",
       "      <td id=\"T_3b31a_row0_col3\" class=\"data row0 col3\" >0.638171</td>\n",
       "      <td id=\"T_3b31a_row0_col4\" class=\"data row0 col4\" >0.873146</td>\n",
       "      <td id=\"T_3b31a_row0_col5\" class=\"data row0 col5\" >0.737375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3b31a_level0_row1\" class=\"row_heading level0 row1\" >Random forest</th>\n",
       "      <td id=\"T_3b31a_row1_col0\" class=\"data row1 col0\" >7.971916</td>\n",
       "      <td id=\"T_3b31a_row1_col1\" class=\"data row1 col1\" >0.124539</td>\n",
       "      <td id=\"T_3b31a_row1_col2\" class=\"data row1 col2\" >0.617968</td>\n",
       "      <td id=\"T_3b31a_row1_col3\" class=\"data row1 col3\" >0.650426</td>\n",
       "      <td id=\"T_3b31a_row1_col4\" class=\"data row1 col4\" >0.786873</td>\n",
       "      <td id=\"T_3b31a_row1_col5\" class=\"data row1 col5\" >0.712139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13db97fd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiers = {\n",
    "    'Logit':LogisticRegression(),\n",
    "    'Random forest':RandomForestClassifier(),\n",
    "    #'SVM':SVC()\n",
    "}\n",
    "\n",
    "scores1 = {} # Store cross-validation results in a dictionary\n",
    "for classifier in classifiers: \n",
    "    scores1[classifier] = cross_validate( # perform cross-validation\n",
    "        classifiers[classifier], # classifier object\n",
    "        X_docterm, # feature matrix\n",
    "        y_year, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy','precision', 'recall', 'f1'] # scoring methods\n",
    "    )\n",
    "    \n",
    "compare_scores(scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6be8bd69-056b-49b4-8adb-ccd2712d2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = f_classif # f is much faster than mutal_info, but not as robust\n",
    "selector = SelectKBest(method, k=5)\n",
    "X_best = selector.fit_transform(X_docterm, y_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f57beb41-cedd-4a9a-8414-ba3f6d0c9da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_89d0e_row0_col0, #T_89d0e_row0_col1, #T_89d0e_row0_col3, #T_89d0e_row1_col2, #T_89d0e_row1_col4, #T_89d0e_row1_col5 {\n",
       "  background-color: #a50026;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_89d0e_row0_col2, #T_89d0e_row0_col4, #T_89d0e_row0_col5, #T_89d0e_row1_col0, #T_89d0e_row1_col1, #T_89d0e_row1_col3 {\n",
       "  background-color: #006837;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_89d0e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_89d0e_level0_col0\" class=\"col_heading level0 col0\" >fit_time</th>\n",
       "      <th id=\"T_89d0e_level0_col1\" class=\"col_heading level0 col1\" >score_time</th>\n",
       "      <th id=\"T_89d0e_level0_col2\" class=\"col_heading level0 col2\" >test_accuracy</th>\n",
       "      <th id=\"T_89d0e_level0_col3\" class=\"col_heading level0 col3\" >test_precision</th>\n",
       "      <th id=\"T_89d0e_level0_col4\" class=\"col_heading level0 col4\" >test_recall</th>\n",
       "      <th id=\"T_89d0e_level0_col5\" class=\"col_heading level0 col5\" >test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_89d0e_level0_row0\" class=\"row_heading level0 row0\" >Logit</th>\n",
       "      <td id=\"T_89d0e_row0_col0\" class=\"data row0 col0\" >0.036715</td>\n",
       "      <td id=\"T_89d0e_row0_col1\" class=\"data row0 col1\" >0.016909</td>\n",
       "      <td id=\"T_89d0e_row0_col2\" class=\"data row0 col2\" >0.622293</td>\n",
       "      <td id=\"T_89d0e_row0_col3\" class=\"data row0 col3\" >0.627345</td>\n",
       "      <td id=\"T_89d0e_row0_col4\" class=\"data row0 col4\" >0.914117</td>\n",
       "      <td id=\"T_89d0e_row0_col5\" class=\"data row0 col5\" >0.744051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_89d0e_level0_row1\" class=\"row_heading level0 row1\" >Random forest</th>\n",
       "      <td id=\"T_89d0e_row1_col0\" class=\"data row1 col0\" >3.555621</td>\n",
       "      <td id=\"T_89d0e_row1_col1\" class=\"data row1 col1\" >0.105877</td>\n",
       "      <td id=\"T_89d0e_row1_col2\" class=\"data row1 col2\" >0.604992</td>\n",
       "      <td id=\"T_89d0e_row1_col3\" class=\"data row1 col3\" >0.633047</td>\n",
       "      <td id=\"T_89d0e_row1_col4\" class=\"data row1 col4\" >0.814365</td>\n",
       "      <td id=\"T_89d0e_row1_col5\" class=\"data row1 col5\" >0.712336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13db64a90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores2 = {} # Store cross-validation results in a dictionary\n",
    "for classifier in classifiers: \n",
    "    scores2[classifier] = cross_validate( # perform cross-validation\n",
    "        classifiers[classifier], # classifier object\n",
    "        X_best, # feature matrix\n",
    "        y_year, # gold labels\n",
    "        cv=10, #number of folds\n",
    "        scoring=['accuracy','precision', 'recall', 'f1'] # scoring methods\n",
    "    )\n",
    "\n",
    "compare_scores(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae51552f-35cd-4da5-a9d0-d38165c60d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 13 students \t\tscore: 961.83154296875\n",
      "Topic 6 room \t\tscore: 474.48876953125\n",
      "Topic 8 day \t\tscore: 261.0123291015625\n",
      "Topic 17 chinese \t\tscore: 177.13648986816406\n",
      "Topic 15 class \t\tscore: 151.72744750976562\n"
     ]
    }
   ],
   "source": [
    "all_features = tomo_df.columns[:n_topics].tolist()\n",
    "top_features = sorted(zip(all_features, selector.scores_), key=lambda x: x[1], reverse=True)\n",
    "for top_feature in top_features[:5]:\n",
    "    print(f'{top_feature[0]} \\t\\tscore: {top_feature[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f5a66-8b3a-4e03-b22e-55955fe3b711",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Permutation test\n",
    "\n",
    "In order to find out whether differences between topic distributions are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf1bf8e9-1b33-4214-a943-c6a63b3518dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bdd1f95f-7e3e-486a-a170-7ed669eb0185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute(input_array):\n",
    "    # shuffle is inplace, so copy to preserve input\n",
    "    permuted = input_array.copy().values  # convert to numpy array, avoiding warning\n",
    "    np.random.shuffle(permuted)\n",
    "    return permuted  # convert back to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74a527f3-3246-4339-9aa3-ec1008c31f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_test(ddf, raw_column):\n",
    "    \n",
    "    # Difference between the mean of the values in the first half and the mean of the values in the second half of the corpus\n",
    "    column = f'{raw_column}_z'\n",
    "    ddf[column] = stats.zscore(ddf[raw_column])\n",
    "    real_mean_before = ddf.loc[ddf['year'] == '2020'][column].mean()\n",
    "    real_mean_after = ddf.loc[ddf['year'] == '2022'][column].mean()\n",
    "    diff_real = real_mean_before - real_mean_after \n",
    "    \n",
    "    # Performing 1,000 permutations\n",
    "    n_permutations = 1000\n",
    "    flag = 0\n",
    "    for i in range(n_permutations):\n",
    "        copy = ddf.copy()  # we copy the original dataframe with the observed data\n",
    "        copy['year'] = permute(copy['year'])  # we shuffle the 'year' column\n",
    "        mean_before = copy.loc[copy['year'] == '2020'][column].mean()\n",
    "        mean_after = copy.loc[copy['year'] == '2022'][column].mean()\n",
    "        diff_perm = mean_before - mean_after  # we calculate the difference between the means of the two halves of the corpus\n",
    "        if diff_real > 0:  # if real difference is a positive number\n",
    "            if diff_real > diff_perm:  # we test if the observed difference is greater\n",
    "                flag += 1\n",
    "        if diff_real < 0:  # if real difference is a positive number\n",
    "            if diff_real < diff_perm:  # we test if the observed difference is lesser\n",
    "                flag += 1  # we keep count of the number of times the observed difference is larger\n",
    "    p = (n_permutations-flag)/n_permutations\n",
    "    \n",
    "    return diff_real, flag, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "588906b5-331f-4d30-b1ef-fb1635db7cd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 year in 2020 vs 2022\n",
      "Observed difference: 0.0941096842288971\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 1 campus in 2020 vs 2022\n",
      "Observed difference: -0.12721779942512512\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 2 know in 2020 vs 2022\n",
      "Observed difference: 0.045727986842393875\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 3 food in 2020 vs 2022\n",
      "Observed difference: -0.07613328099250793\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 4 email in 2020 vs 2022\n",
      "Observed difference: -0.030781012028455734\n",
      "Number of times observed difference is larger than permutated: 997\n",
      "P-value: 0.003\n",
      "\n",
      "Topic 5 covid in 2020 vs 2022\n",
      "Observed difference: -0.014654358848929405\n",
      "Number of times observed difference is larger than permutated: 908\n",
      "P-value: 0.092\n",
      "\n",
      "Topic 6 room in 2020 vs 2022\n",
      "Observed difference: -0.23878951370716095\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 7 think in 2020 vs 2022\n",
      "Observed difference: -0.009211577475070953\n",
      "Number of times observed difference is larger than permutated: 795\n",
      "P-value: 0.205\n",
      "\n",
      "Topic 8 day in 2020 vs 2022\n",
      "Observed difference: -0.1776566207408905\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 9 really in 2020 vs 2022\n",
      "Observed difference: -0.04173607379198074\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 10 campus in 2020 vs 2022\n",
      "Observed difference: 0.08119502663612366\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 11 take in 2020 vs 2022\n",
      "Observed difference: 0.08264985680580139\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 12 https in 2020 vs 2022\n",
      "Observed difference: -0.029980503022670746\n",
      "Number of times observed difference is larger than permutated: 993\n",
      "P-value: 0.007\n",
      "\n",
      "Topic 13 students in 2020 vs 2022\n",
      "Observed difference: 0.3376154899597168\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 14 school in 2020 vs 2022\n",
      "Observed difference: -0.00851446483284235\n",
      "Number of times observed difference is larger than permutated: 776\n",
      "P-value: 0.224\n",
      "\n",
      "Topic 15 class in 2020 vs 2022\n",
      "Observed difference: 0.13566306233406067\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 16 time in 2020 vs 2022\n",
      "Observed difference: -0.034411221742630005\n",
      "Number of times observed difference is larger than permutated: 998\n",
      "P-value: 0.002\n",
      "\n",
      "Topic 17 chinese in 2020 vs 2022\n",
      "Observed difference: -0.14652974903583527\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n",
      "Topic 18 think in 2020 vs 2022\n",
      "Observed difference: 0.00082982680760324\n",
      "Number of times observed difference is larger than permutated: 549\n",
      "P-value: 0.451\n",
      "\n",
      "Topic 19 major in 2020 vs 2022\n",
      "Observed difference: 0.1188708245754242\n",
      "Number of times observed difference is larger than permutated: 1000\n",
      "P-value: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Permutation test on the difference between the daily relative occurence of the symptoms label\n",
    "# in the first and second halves of the corpus\n",
    "for _column in tomo_df.columns[:n_topics]:\n",
    "    diff, flag_value, p_value = permutation_test(tomo_df, _column)\n",
    "    print(f'{_column} in 2020 vs 2022')\n",
    "    print(f'Observed difference: {diff}')\n",
    "    print(f'Number of times observed difference is larger than permutated: {flag_value}')\n",
    "    print(f'P-value: {p_value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7770847f-0e34-4344-9a36-c6f67c41fd28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
